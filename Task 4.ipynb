{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c38be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f4577",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = pd.read_csv('month.csv')\n",
    "real_estate_df = pd.read_csv('Real_Estate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed2726",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba128ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_estate_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37570e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame for the grouped and aggregated data\n",
    "grouped_real_estate_df = real_estate_df.groupby(['YEAR', 'LocationID'])['FULLVAL'].sum().reset_index()\n",
    "grouped_real_estate_df.head()\n",
    "\n",
    "taxi_df['Date'] = pd.to_datetime(taxi_df[\"Date\"])\n",
    "taxi_df[\"YEAR\"] = taxi_df['Date'].dt.year\n",
    "\n",
    "grouped_taxi_df = taxi_df.groupby(['LocationID', 'YEAR']).agg({\n",
    "    'PU_passenger': 'sum',\n",
    "    'PUTripDistance_count': 'sum',\n",
    "    'PickUp_count': 'sum',\n",
    "    'PU_fare': 'sum',\n",
    "    'DO_passenger': 'sum',\n",
    "    'DropOff_count': 'sum',\n",
    "    'DO_fare': 'sum',\n",
    "    'DOT_fare': 'sum',\n",
    "    'DOTripDistance_count': 'sum',\n",
    "}).reset_index()\n",
    "\n",
    "taxi_data_filtered = grouped_taxi_df[(grouped_taxi_df['YEAR'] >= 2017) &\n",
    "                                     (grouped_taxi_df['YEAR'] <= 2019)]\n",
    "real_estate_filtered = grouped_real_estate_df[(grouped_real_estate_df['YEAR'] >= 2017) & \n",
    "                                             (grouped_real_estate_df['YEAR'] <= 2019)]\n",
    "\n",
    "#merging the two filtered data sets\n",
    "merged_df = pd.merge(taxi_data_filtered, real_estate_filtered, on= ['LocationID', 'YEAR'], \n",
    "                     suffixes = ('_taxi', '_real_estate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8f9465",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['passenger'] = merged_df['DO_passenger'] + merged_df['PU_passenger']\n",
    "merged_df['fare'] = merged_df['DO_fare'] + merged_df['PU_fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df[['passenger', 'fare', 'DO_passenger', 'DropOff_count']]\n",
    "y = merged_df['FULLVAL']\n",
    "\n",
    "# Add a constant to the independent variables matrix\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the OLS model\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d303c1",
   "metadata": {},
   "source": [
    "\n",
    "1. **Dep. Variable**: The dependent variable in the regression analysis is `FULLVAL`.\n",
    "\n",
    "2. **Model**: The model is an Ordinary Least Squares (OLS) regression.\n",
    "\n",
    "3. **Method**: The method used for regression is Least Squares.\n",
    "\n",
    "4. **Date and Time**: The date and time when the analysis was performed.\n",
    "\n",
    "5. **No. Observations**: The number of observations or data points used in the regression analysis is 132.\n",
    "\n",
    "6. **R-squared**: The coefficient of determination (R-squared) measures how well the independent variables explain the variability in the dependent variable. In this case, it's 0.840, which means that approximately 84% of the variability in `FULLVAL` can be explained by the independent variables in the model.\n",
    "\n",
    "7. **Adj. R-squared**: The adjusted R-squared adjusts the R-squared value based on the number of independent variables in the model. It's 0.835 here.\n",
    "\n",
    "8. **F-statistic**: The F-statistic tests the overall significance of the model. A larger F-statistic suggests that at least one independent variable is significantly related to the dependent variable. The value here is 167.2, and the associated probability (Prob (F-statistic)) is very close to zero, indicating that the model as a whole is statistically significant.\n",
    "\n",
    "9. **P-values (P>|t|)**: For each independent variable, the p-value tests the null hypothesis that the coefficient of the variable is zero (i.e., the variable has no effect on the dependent variable). If the p-value is small (typically below 0.05), you can reject the null hypothesis and conclude that the variable is statistically significant. In this case, all four independent variables have very small p-values, indicating their significance.\n",
    "\n",
    "10. **Coefficient (coef)**: The coefficients represent the estimated change in the dependent variable for a one-unit change in the corresponding independent variable, holding other variables constant. For example, for the 'passenger' variable, a one-unit increase in 'passenger' is associated with an estimated decrease of approximately $3497 in 'FULLVAL', when other variables are held constant.\n",
    "\n",
    "11. **Standard Error (std err)**: This measures the variability of the coefficient estimate. Smaller standard errors indicate more precise estimates.\n",
    "\n",
    "12. **t-statistic**: The t-statistic is the coefficient divided by its standard error. It measures the number of standard deviations the coefficient estimate is away from zero. Larger absolute t-values suggest stronger evidence against the null hypothesis.\n",
    "\n",
    "13. **Omnibus, Prob(Omnibus), Skew, Kurtosis**: These are tests and statistics related to the distribution of residuals. The Omnibus test tests the normality of residuals. Prob(Omnibus) is the associated p-value. Skew measures the symmetry of the residuals distribution, and Kurtosis measures the \"tailedness\" of the distribution. Low p-values in Omnibus and high values of Skew and Kurtosis may suggest that the residuals are not normally distributed.\n",
    "\n",
    "14. **Durbin-Watson**: This test helps detect the presence of autocorrelation (dependence between residuals at different time points). The value here is close to 2, which suggests a lack of significant positive autocorrelation.\n",
    "\n",
    "15. **Cond. No. (Condition Number)**: This tests multicollinearity, which is the presence of high correlation between independent variables. A high condition number may indicate multicollinearity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2746d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_location_ids = merged_df['LocationID'].unique()\n",
    "# Create an empty dictionary\n",
    "coef_dict = {}\n",
    "\n",
    "# Loop over each unique LocationID\n",
    "for location_id in merged_df['LocationID'].unique():\n",
    "    # Filter data for the current LocationID\n",
    "    group_data = merged_df[merged_df['LocationID'] == location_id]\n",
    "    \n",
    "    # Define predictor variables (X) and target variable (y)\n",
    "    X = group_data[['passenger', 'fare', 'DO_passenger', 'DropOff_count']]\n",
    "    y = group_data['FULLVAL']\n",
    "    \n",
    "    # Add a constant to the predictor variables\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Create and fit the OLS model\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "    \n",
    "    # Store the regression coefficients in the dictionary\n",
    "    coef_dict[location_id] = results.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f77660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020 = pd.read_csv('Month_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7959afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020['passenger'] = df_2020['PU_passenger'] + df_2020['DO_passenger']\n",
    "df_2020['fare'] = df_2020['PU_fare'] + df_2020['DO_fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b201e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date column to datetime type\n",
    "df_2020['Date'] = pd.to_datetime(df_2020['Date'])\n",
    "\n",
    "# Record the error rates of each model\n",
    "avg_error_rates = []\n",
    "min_error_rates = []\n",
    "\n",
    "values_to_exclude = []\n",
    "\n",
    "# Obtain a unique LocationID list\n",
    "unique_location_ids = df_2020['LocationID'].unique()\n",
    "\n",
    "# Use the delete function from numpy to exclude specific values.\n",
    "updated_location_ids = np.delete(unique_location_ids, np.where(np.isin(unique_location_ids, values_to_exclude)))\n",
    "\n",
    "# Define ARIMA model parameters\n",
    "order = (1, 1, 2)  # (AR order, difference order, MA order)\n",
    "\n",
    "# Predict and plot data for each LocationID\n",
    "for loc_id in unique_location_ids:\n",
    "    # Filter data based on LocationID\n",
    "    loc_data = df_2020[df_2020['LocationID'] == loc_id]\n",
    "\n",
    "    # Split the dataset into training and testing data\n",
    "    train_size = int(len(loc_data) * 0.8)\n",
    "    train_data = loc_data.iloc[:train_size]\n",
    "    test_data = loc_data.iloc[train_size:]\n",
    "\n",
    "    # Extracting training data and testing data\n",
    "    train_passenger = train_data['passenger'].values\n",
    "    train_dropoff_count = train_data['DropOff_count'].values\n",
    "\n",
    "    train_features = np.column_stack((train_passenger, train_dropoff_count))\n",
    "    train_target = train_data['fare'].values\n",
    "\n",
    "    # Train an ARIMA model\n",
    "    model = sm.tsa.ARIMA(train_target, exog=train_features, order=order)\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    # Make predictions\n",
    "    test_features = np.column_stack((test_data['passenger'].values, test_data['DropOff_count'].values))\n",
    "    test_predictions_fare = model_fit.predict(start=train_size, end=len(loc_data) - 1, exog=test_features)\n",
    "\n",
    "    # Calculate the error rate for the predictions\n",
    "    error_rate = np.abs(test_predictions_fare - test_data['fare'].values) / test_data['fare'].values\n",
    "    avg_error_rate = np.mean(error_rate)\n",
    "\n",
    "    avg_error_rates.append(avg_error_rate)\n",
    "    min_error_rates.append(np.min(error_rate))\n",
    "\n",
    "    # Draw Line chart\n",
    "    plt.plot(train_data['Date'], train_target, label='Train Actual')\n",
    "    plt.plot(test_data['Date'], test_data['fare'].values, label='Test Actual')\n",
    "    plt.plot(test_data['Date'], test_predictions_fare, label='Test Predictions')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('fare')\n",
    "    plt.title(f'LocationID {loc_id} - fare Forecast\\nAverage Error Rate: {avg_error_rate:.2%}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Print the average and minimum error rates\n",
    "print(\"Average Error Rates:\", avg_error_rates)\n",
    "print(\"Minimum Error Rates:\", min_error_rates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d107e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_without_extremes(arr):\n",
    "    if len(arr) <= 2:\n",
    "        return None  # The number of elements in the array is less than or equal to 2; the maximum and minimum values cannot be discarded\n",
    "\n",
    "    # Sort the array\n",
    "    sorted_arr = sorted(arr)\n",
    "\n",
    "    # Calculate the trimmed mean by excluding the first and last elements\n",
    "    trimmed_mean = sum(sorted_arr[1:-1]) / (len(sorted_arr) - 2)\n",
    "    return trimmed_mean\n",
    "\n",
    "print(\"Model best-case average error rate:\",str(calculate_mean_without_extremes(min_error_rates)))\n",
    "print(\"The greatest model error rate：\", str(min(avg_error_rates)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_exclude = []\n",
    "\n",
    "# Get a list of unique LocationIDs\n",
    "unique_locs = df_2020['LocationID'].unique()\n",
    "\n",
    "# Use numpy to exclude specific values\n",
    "remaining_locs = np.delete(unique_locs, np.where(np.isin(unique_locs, values_to_exclude)))\n",
    "\n",
    "# Define ARIMA model parameters\n",
    "order = (2, 1, 2)  # (AR order, difference order, MA order)\n",
    "\n",
    "# Create a dictionary to store models\n",
    "arima_models = {}\n",
    "\n",
    "# Predict and plot data for each LocationID\n",
    "for loc_id in remaining_locs:\n",
    "    # Filter data based on LocationID\n",
    "    loc_data = df_2020[df_2020['LocationID'] == loc_id]\n",
    "\n",
    "    # Extract training data and testing data\n",
    "    train_passenger = loc_data['passenger'].values\n",
    "    train_DOcount = loc_data['DropOff_count'].values\n",
    "    \n",
    "    train_features = np.column_stack((train_passenger, train_DOcount))\n",
    "    train_target = loc_data['DO_passenger'].values\n",
    "\n",
    "    # Train an ARIMA model\n",
    "    model = ARIMA(train_target, exog=train_features, order=order)\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    # Add the model to the dictionary\n",
    "    arima_models[loc_id] = model_fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3bde04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file 'Task_4.py' in read mode\n",
    "with open('Task_4.py', 'r') as f:\n",
    "    # Read the content of the 'Task_4.py' file and store it in the variable 'notebook_code'\n",
    "    notebook_code = f.read()\n",
    "\n",
    "# Create the file 'Task_4.pkl' in binary write mode\n",
    "with open('Task_4.pkl', 'wb') as f:\n",
    "    # Use the 'pickle.dump()' function to serialize and save the content of 'notebook_code'\n",
    "    # into the pickle file 'Task_4.pkl'\n",
    "    pickle.dump(notebook_code, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
